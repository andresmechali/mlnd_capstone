{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Project\n",
    "Andres Mechali\n",
    "\n",
    "May 13th, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is taken from the Kaggle platform, and is currently available under the name _Quora Question Pairs_ in this link: https://www.kaggle.com/c/quora-question-pairs. Quora is a platform where anyone can make a question, and other users give answers. According to Quora, they receive over 100 million visits every month, so it's very common that the question someone asks has already been answered by others. In order to make it easier for someone to find an answer, they use a Random Forest model to identify duplicate questions.\n",
    "\n",
    "This project is about finding a model that can determine wether a pair of questions has the same meaning or not. The input data is a dataset of about 400,000 pair of questions with a human provided label stating if they have the same meaning. As for testing, Kaggle provides a set of unlabeled pairs, which are then compared to their own human labeled results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "In this section, you will want to clearly define the problem that you are trying to solve, including the strategy (outline of tasks) you will use to achieve the desired solution. You should also thoroughly discuss what the intended solution will be for this problem. Questions to ask yourself when writing this section:\n",
    "- _Is the problem statement clearly defined? Will the reader understand what you are expecting to solve?_\n",
    "- _Have you thoroughly discussed how you will attempt to solve the problem?_\n",
    "- _Is an anticipated solution clearly defined? Will the reader understand what results you are looking for?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FALTA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics that I will use for measuring the performance of the models will be the log-loss. This is a very commonly used metrics in models where the output is the probability of a binary outcome, which is the case of this project. This metrics penalize wrong classifications depending on how confident the model was about them. Specifically, a prediction with a greater level of confidence is more penalized when wrong.\n",
    "\n",
    "Another reason for choosing log-loss as a metrics is because it is the one used in the Kaggle platform to evaluate the predictions. So, by using this while training, I can compare the results with other Kaggle users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, I need to import the basic libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will import the training set, and display the first 5 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv('data/train.csv')\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset we can find the following information:\n",
    "* qid1: Unique ID of question 1\n",
    "* qid2: Unique ID of question 2\n",
    "* question1: Content of question 1\n",
    "* question2: Content of question 2\n",
    "* is_duplicate: A label stating if both questions are the same (1) or not (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will display some basic statistics about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 404290\n",
      "Duplicate pair of questions: 149263\n",
      "Not duplicate pair of questions: 255027\n",
      "Unique questions: 537933\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows: %d\" %len(data_train))\n",
    "dup = data_train[data_train.is_duplicate == 1]\n",
    "n_dup = data_train[data_train.is_duplicate == 0]\n",
    "print(\"Duplicate pair of questions: %d\" %len(dup))\n",
    "print(\"Not duplicate pair of questions: %d\" %len(data_train[data_train.is_duplicate == 0]))\n",
    "\n",
    "qids = data_train.qid1.tolist() + data_train.qid2.tolist()\n",
    "print(\"Unique questions: %d\" %len(set(qids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will do a brief analysis of the questions in the training set. I will use the Natural Language Tool Kit (nltk) for this, and I will display an histogram to see the frequency of the lengths of the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA74AAAIvCAYAAABTMwdTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9ua9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x31d31908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "q_tok = []\n",
    "q_tok += [nltk.tokenize.word_tokenize(i) for i in data_train.question1.apply(lambda x: str(x).decode('utf-8'))]\n",
    "q_tok += [nltk.tokenize.word_tokenize(i) for i in data_train.question2.apply(lambda x: str(x).decode('utf-8'))]\n",
    "\n",
    "word_count = [len(i) for i in q_tok]\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.hist(word_count, bins=30, range=[0,70], normed=True)\n",
    "plt.title(\"Normalized histogram of word count\", fontsize=20)\n",
    "plt.xlabel(\"Number of words\", fontsize=14)\n",
    "plt.ylabel(\"Appearances\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we might see here, most of the questions are composed from about 8 to 12 words. There are some of them which can have even more than 50 words. I will not exclude these outliers, because when comparing lengths I will use the absolute value, and not a normalized one. Hence, outliers should not affect my outcome. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project I will evaluate 3 algorithms: AdaBoost, Random Forest and XGBoost. The reason for this is that they are all ensemble methods, which work well for problems where the features are weak learners. In this problem all the features will be so, because there doesn't seem to be one that is highly correlated with the meaning of the question.\n",
    "\n",
    "I will try the 3 models and determine which has the minimum log-loss. After that, I will tune this model to find the best possible parameters for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark: Naive predictor\n",
    "As a Benchmark for testing the effectiveness of this model, I will use a predictor that evaluates if two questions are equal just based on the probability of this happening. I will assign this same probability to every pair of questions. I will upload this result to the Kaggle platform an evaluate the log-loss obtained. This will be my benchmark to see if I can obtain a better model for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob = data_train.is_duplicate.mean()\n",
    "\n",
    "data_test = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data_test.test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_is_duplicate = []\n",
    "\n",
    "for i in range(len(submission.test_id)):\n",
    "    test_is_duplicate.append(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.369198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.369198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.369198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.369198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.369198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id  is_duplicate\n",
       "0        0      0.369198\n",
       "1        1      0.369198\n",
       "2        2      0.369198\n",
       "3        3      0.369198\n",
       "4        4      0.369198"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['is_duplicate'] = test_is_duplicate\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('sub/naive_predictor.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-loss obtained was **0.55411**. This will be used as a benchmark, and any model with a lower value will be considered better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now pre process the raw data, for which I will need a series of steps stated below:\n",
    "\n",
    "* Removing stop words: The first step will be to remove all words that are considered not to give valuable information about the content of the question. Examples of this will be _you_, _have_, _the_, _to_, etc., and punctuation.\n",
    "* Applying stemming: In order to consider the same word _car_ and _cars_, I will apply a stemming algorithm that keeps the root of each word.\n",
    "* Tokenize: I will split questions into list of words, so that I can loop and compare them easily.\n",
    "* Tf-Idf: With this I will consider the importance of a word related to it's frequency, instead of considering them equal. The fact that 2 question share a word that appears often will be less important than if they share a rare word.\n",
    "* DiffLib: This is a Python library that will help me finding differences between 2 strings. Specifically, I will be using the SequenceMatcher method.\n",
    "\n",
    "By doing these steps, I will get a new set of features, which will be composed of:\n",
    "* shared_weights: a measure of how many words both questions share, weighted by the tf-idf value of each word.\n",
    "* shared_count_scaled: it will represent the normalized amount of words that they have in common.\n",
    "* len_dif: the difference between the lengths of the questions.\n",
    "* z_match_ratio: the value obtained from the SequenceMatcher method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train = data_train[0:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "In this section, the process for which metrics, algorithms, and techniques that you implemented for the given data will need to be clearly documented. It should be abundantly clear how the implementation was carried out, and discussion should be made regarding any complications that occurred during this process. Questions to ask yourself when writing this section:\n",
    "- _Is it made clear how the algorithms and techniques were implemented with the given datasets or input data?_\n",
    "- _Were there any complications with the original metrics or techniques that required changing prior to acquiring a solution?_\n",
    "- _Was there any part of the coding process (e.g., writing complicated functions) that should be documented?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement\n",
    "In this section, you will need to discuss the process of improvement you made upon the algorithms and techniques you used in your implementation. For example, adjusting parameters for certain models to acquire improved solutions would fall under the refinement category. Your initial and final solutions should be reported, as well as any significant intermediate results as necessary. Questions to ask yourself when writing this section:\n",
    "- _Has an initial solution been found and clearly reported?_\n",
    "- _Is the process of improvement clearly documented, such as what techniques were used?_\n",
    "- _Are intermediate and final solutions clearly reported as the process is improved?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and Validation\n",
    "In this section, the final model and any supporting qualities should be evaluated in detail. It should be clear how the final model was derived and why this model was chosen. In addition, some type of analysis should be used to validate the robustness of this model and its solution, such as manipulating the input data or environment to see how the model’s solution is affected (this is called sensitivity analysis). Questions to ask yourself when writing this section:\n",
    "- _Is the final model reasonable and aligning with solution expectations? Are the final parameters of the model appropriate?_\n",
    "- _Has the final model been tested with various inputs to evaluate whether the model generalizes well to unseen data?_\n",
    "- _Is the model robust enough for the problem? Do small perturbations (changes) in training data or the input space greatly affect the results?_\n",
    "- _Can results found from the model be trusted?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification\n",
    "In this section, your model’s final solution and its results should be compared to the benchmark you established earlier in the project using some type of statistical analysis. You should also justify whether these results and the solution are significant enough to have solved the problem posed in the project. Questions to ask yourself when writing this section:\n",
    "- _Are the final results found stronger than the benchmark result reported earlier?_\n",
    "- _Have you thoroughly analyzed and discussed the final solution?_\n",
    "- _Is the final solution significant enough to have solved the problem?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free-Form Visualization\n",
    "In this section, you will need to provide some form of visualization that emphasizes an important quality about the project. It is much more free-form, but should reasonably support a significant result or characteristic about the problem that you want to discuss. Questions to ask yourself when writing this section:\n",
    "- _Have you visualized a relevant or important quality about the problem, dataset, input data, or results?_\n",
    "- _Is the visualization thoroughly analyzed and discussed?_\n",
    "- _If a plot is provided, are the axes, title, and datum clearly defined?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "In this section, you will summarize the entire end-to-end problem solution and discuss one or two particular aspects of the project you found interesting or difficult. You are expected to reflect on the project as a whole to show that you have a firm understanding of the entire process employed in your work. Questions to ask yourself when writing this section:\n",
    "- _Have you thoroughly summarized the entire process you used for this project?_\n",
    "- _Were there any interesting aspects of the project?_\n",
    "- _Were there any difficult aspects of the project?_\n",
    "- _Does the final model and solution fit your expectations for the problem, and should it be used in a general setting to solve these types of problems?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement\n",
    "In this section, you will need to provide discussion as to how one aspect of the implementation you designed could be improved. As an example, consider ways your implementation can be made more general, and what would need to be modified. You do not need to make this improvement, but the potential solutions resulting from these changes are considered and compared/contrasted to your current solution. Questions to ask yourself when writing this section:\n",
    "- _Are there further improvements that could be made on the algorithms or techniques you used in this project?_\n",
    "- _Were there algorithms or techniques you researched that you did not know how to implement, but would consider using if you knew how?_\n",
    "- _If you used your final solution as the new benchmark, do you think an even better solution exists?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better analysis, I will remove stopwords and apply stemming to each questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define stemmer method and stop words\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}']) # Adds punctuation to stopwords\n",
    "\n",
    "\n",
    "def cleanQuestions(q_list):\n",
    "    clean_questions = []\n",
    "\n",
    "    for i in range(len(q_list)):\n",
    "        q1 = ''\n",
    "        for j in re.findall(r\"[\\w']+\", str(q_list[i])):\n",
    "            if j not in stop_words:\n",
    "                q1 += stemmer.stem(j)\n",
    "                q1 += \" \"\n",
    "        q = ''\n",
    "        for j in q1.split(): # Because the first words where not taken before\n",
    "            if j not in stop_words:\n",
    "                q += j\n",
    "                q += \" \"\n",
    "        clean_questions.append(q.strip()) # To remove the last blank space\n",
    "        \n",
    "    return clean_questions\n",
    "\n",
    "questions1 = data_train.question1.tolist()\n",
    "questions2 = data_train.question2.tolist()\n",
    "    \n",
    "clean_questions = {'question1_clean': cleanQuestions(questions1), 'question2_clean': cleanQuestions(questions2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf\n",
    "\n",
    "Now I will apply a TF-IDF model in order to get the relative importance of each word. In this way, words which appear more often will be less important than words that appear less.\n",
    "\n",
    "First, I need to identify the **unique questions**, because some of them are repeated in the dataset and otherwise would generate a different result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 509543 unique questions in the dataset.\n"
     ]
    }
   ],
   "source": [
    "questions1_clean = clean_questions['question1_clean']\n",
    "questions2_clean = clean_questions['question2_clean']\n",
    "\n",
    "questions = []\n",
    "\n",
    "for i in range(len(questions1_clean)):\n",
    "    questions.append(str(questions1_clean[i]))\n",
    "    questions.append(str(questions2_clean[i]))\n",
    "    \n",
    "questions = set(questions)\n",
    "\n",
    "print(\"There are %d unique questions in the dataset.\" %len(questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now **tokenize** each question, in order to make it easier for looping through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_tokens = [nltk.tokenize.word_tokenize(str(i)) for i in questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will obtain the Tf-Idf value of each word, using the TfidfVectorizer function from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5)\n",
    "\n",
    "tfidf = vectorizer.fit_transform(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now create a dictionary with every word as a key, and its respective weight as a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf = vectorizer.idf_\n",
    "idf_dict = dict(zip(vectorizer.get_feature_names(), idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will compute a new variable, which will represent the sum of the share words weights, divided by the sum of all the words of a question pair, to determine how much do they have in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question1_token = [nltk.tokenize.word_tokenize(i) for i in questions1_clean]\n",
    "question2_token = [nltk.tokenize.word_tokenize(i) for i in questions2_clean]\n",
    "\n",
    "shared_words = []\n",
    "total_words = []\n",
    "\n",
    "for i in range(len(question1_token)):\n",
    "    shared = []\n",
    "    total = []\n",
    "    for w in question1_token[i]:\n",
    "        total.append(w)\n",
    "        if w in question2_token[i]:\n",
    "            shared.append(w)\n",
    "    for w in question2_token[i]:\n",
    "        total.append(w)\n",
    "        if w in question1_token[i]:\n",
    "            shared.append(w)\n",
    "    shared_words.append(shared)\n",
    "    total_words.append(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now create 2 new lists with the shared and total weights, using the idf dictionary created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shared_weights = []\n",
    "total_weights = []\n",
    "not_in_dict = []\n",
    "\n",
    "for i in range(len(shared_words)):\n",
    "    shared = []\n",
    "    total = []\n",
    "    for w in shared_words[i]:\n",
    "        try:\n",
    "            shared.append(idf_dict[w])\n",
    "        except:\n",
    "            not_in_dict.append(w)\n",
    "            shared.append(0)\n",
    "    for w in total_words[i]:\n",
    "        try:\n",
    "            total.append(idf_dict[w])\n",
    "        except:\n",
    "            not_in_dict.append(w)\n",
    "            shared.append(0)\n",
    "    shared_weights.append(shared)\n",
    "    total_weights.append(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I need a list that has the sum of all the shared weights divided by the sum of all the weights for each pair of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = []\n",
    "\n",
    "for i in range(len(shared_weights)):\n",
    "    shared = 0\n",
    "    total = 0\n",
    "    for j in range(len(shared_weights[i])):\n",
    "        shared += shared_weights[i][j]\n",
    "    for j in range(len(total_weights[i])):\n",
    "        total += total_weights[i][j]\n",
    "    if total == 0:\n",
    "        weights.append(0)\n",
    "    else:\n",
    "        weights.append(shared / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will start a new DataFrame with the features I create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shared_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.950408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.652899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.517698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.237645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shared_weights\n",
       "0        0.950408\n",
       "1        0.652899\n",
       "2        0.517698\n",
       "3        0.000000\n",
       "4        0.237645"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame(weights, columns=['shared_weights'])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next feature I will generate will be the number of shared words in both questions. In order to make it more relevant, I will use the MinMaxScaler function of sklearn, for scaling this feature before using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\andres\\anaconda2\\envs\\udacity\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "c:\\users\\andres\\anaconda2\\envs\\udacity\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shared_weights</th>\n",
       "      <th>shared_count_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.950408</td>\n",
       "      <td>0.206897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.652899</td>\n",
       "      <td>0.137931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.517698</td>\n",
       "      <td>0.103448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.237645</td>\n",
       "      <td>0.068966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shared_weights  shared_count_scaled\n",
       "0        0.950408             0.206897\n",
       "1        0.652899             0.137931\n",
       "2        0.517698             0.103448\n",
       "3        0.000000             0.000000\n",
       "4        0.237645             0.068966"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "shared_count = [len(x) for x in shared_words]\n",
    "shared_count_scaled = MinMaxScaler().fit_transform(shared_count)\n",
    "\n",
    "X['shared_count_scaled'] = shared_count_scaled\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next and last feature I will add is the difference in the length of the question. Again, I will scale it to get a value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\andres\\anaconda2\\envs\\udacity\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "c:\\users\\andres\\anaconda2\\envs\\udacity\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shared_weights</th>\n",
       "      <th>shared_count_scaled</th>\n",
       "      <th>len_dif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.950408</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.009434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.652899</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.047170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.517698</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.009434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.237645</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.047170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shared_weights  shared_count_scaled   len_dif\n",
       "0        0.950408             0.206897  0.009434\n",
       "1        0.652899             0.137931  0.047170\n",
       "2        0.517698             0.103448  0.009434\n",
       "3        0.000000             0.000000  0.047170\n",
       "4        0.237645             0.068966  0.047170"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question1_len = [len(x) for x in question1_token]\n",
    "question2_len = [len(x) for x in question2_token]\n",
    "\n",
    "len_dif = []\n",
    "\n",
    "for i in range(len(question1_len)):\n",
    "    len_dif.append(abs(question1_len[i] - question2_len[i]))\n",
    "\n",
    "len_dif_scaled = MinMaxScaler().fit_transform(len_dif)\n",
    "\n",
    "X['len_dif'] = len_dif_scaled\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to test the algorithms, I will now generate a new DataFrame with the label for each pair of questions. This label will have value 0 if they don't mean the same, and 1 if they do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DiffLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "def diff_ratios(st1, st2):\n",
    "    seq = difflib.SequenceMatcher()\n",
    "    seq.set_seqs(str(st1).lower(), str(st2).lower())\n",
    "    return seq.ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 404290/404290 [02:27<00:00, 2741.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shared_weights</th>\n",
       "      <th>shared_count_scaled</th>\n",
       "      <th>len_dif</th>\n",
       "      <th>z_match_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.950408</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.926829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.652899</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0.661871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.517698</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.439394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0.086957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.237645</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0.365217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shared_weights  shared_count_scaled   len_dif  z_match_ratio\n",
       "0        0.950408             0.206897  0.009434       0.926829\n",
       "1        0.652899             0.137931  0.047170       0.661871\n",
       "2        0.517698             0.103448  0.009434       0.439394\n",
       "3        0.000000             0.000000  0.047170       0.086957\n",
       "4        0.237645             0.068966  0.047170       0.365217"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_match_ratio = []\n",
    "\n",
    "for i in tqdm(range(len(data_train))):\n",
    "    z_match_ratio.append(diff_ratios(data_train.question1[i], data_train.question2[i]))\n",
    "                         \n",
    "X['z_match_ratio'] = z_match_ratio\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = pd.DataFrame(data_train.is_duplicate, columns=['is_duplicate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I will split the features and labels into a training and test set, in order to look for a good model. At the end, when I finally decide which to use, I will use the whole set for training, and I will test using the Kaggle testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have both the features that I will try, and the labels for each data point, I will start with the analysis of 3 different models which might obtain a good result: Random forest, AdaBoost and XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=100, random_state=50)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=100, random_state=50)\n",
    "clf.fit(x_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 10.242394\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "pred = clf.predict(x_test)\n",
    "\n",
    "print 'Log loss: %f' %log_loss(y_test, pred, labels=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf= RandomForestClassifier()\n",
    "\n",
    "clf.fit(x_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 10.337217\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(x_test)\n",
    "\n",
    "print 'Log loss: %f' %log_loss(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-5.3.0-posix-seh-rt_v4-rev0\\\\mingw64\\\\bin'\n",
    "\n",
    "os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.687572\ttest-logloss:0.687664\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[10]\ttrain-logloss:0.641881\ttest-logloss:0.64272\n",
      "[20]\ttrain-logloss:0.609619\ttest-logloss:0.611001\n",
      "[30]\ttrain-logloss:0.586152\ttest-logloss:0.58795\n",
      "[40]\ttrain-logloss:0.568597\ttest-logloss:0.570729\n",
      "[50]\ttrain-logloss:0.554947\ttest-logloss:0.55736\n",
      "[60]\ttrain-logloss:0.544419\ttest-logloss:0.547051\n",
      "[70]\ttrain-logloss:0.536192\ttest-logloss:0.538996\n",
      "[80]\ttrain-logloss:0.529844\ttest-logloss:0.53277\n",
      "[90]\ttrain-logloss:0.524733\ttest-logloss:0.527774\n",
      "[99]\ttrain-logloss:0.520933\ttest-logloss:0.524069\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 4\n",
    "\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_test = xgb.DMatrix(x_test, label=y_test)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_test, 'test')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 100, watchlist, early_stopping_rounds=50, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "As we can see, both AdaBoost and Random Forest don't work well with these features, having both a log loss greater than 10. XGBoost, on the other hand, gives a log loss of 0.52 without even having to tune it. So I will work with this model, trying to get the lowest possible log loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing positive an negative cases\n",
    "\n",
    "Based on this article from Kaggle and it's comments, I know that the distribution of possitive and negative cases in the test data is not the same as the one in the training data: https://www.kaggle.com/davidthaler/quora-question-pairs/how-many-1-s-are-in-the-public-lb/run/1013730. In the training data we have around 37% of positive cases, while the test data has around 16.5%.\n",
    "So for having a better score from Kaggle I need to rebalance the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unbalanced ratio: 0.369198\n",
      "Balanced ratio: 0.164997 \n"
     ]
    }
   ],
   "source": [
    "pos_train = X[y.is_duplicate == 1]\n",
    "neg_train = X[y.is_duplicate == 0]\n",
    "\n",
    "p = 0.165 # target ratio\n",
    "\n",
    "n = 0\n",
    "\n",
    "print 'Unbalanced ratio: %f' %(1.0 * len(pos_train) / (len(pos_train) + len(neg_train)))\n",
    "\n",
    "while 1.0 * len(pos_train) / (len(pos_train) + len(neg_train)) >= p:\n",
    "    if n < len(neg_train):\n",
    "        neg_train = neg_train.append(neg_train[n:n+50])\n",
    "        n += 50\n",
    "    \n",
    "print 'Balanced ratio: %f ' %(1.0 * len(pos_train) / (len(pos_train) + len(neg_train)))\n",
    "\n",
    "X_r = pd.concat([pos_train, neg_train])\n",
    "y_r = (np.zeros(len(pos_train)) + 1).tolist() + np.zeros(len(neg_train)).tolist()\n",
    "del pos_train, neg_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the sample is rebalanced, I will split it in training and testing set, like before, and apply the XGBoost algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_r, y_r, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.682663\ttest-logloss:0.682664\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[10]\ttrain-logloss:0.59724\ttest-logloss:0.597264\n",
      "[20]\ttrain-logloss:0.53744\ttest-logloss:0.537484\n",
      "[30]\ttrain-logloss:0.494404\ttest-logloss:0.494472\n",
      "[40]\ttrain-logloss:0.462754\ttest-logloss:0.462846\n",
      "[50]\ttrain-logloss:0.439075\ttest-logloss:0.439204\n",
      "[60]\ttrain-logloss:0.421191\ttest-logloss:0.42137\n",
      "[70]\ttrain-logloss:0.407483\ttest-logloss:0.407713\n",
      "[80]\ttrain-logloss:0.397035\ttest-logloss:0.397332\n",
      "[90]\ttrain-logloss:0.388999\ttest-logloss:0.389363\n",
      "[99]\ttrain-logloss:0.383314\ttest-logloss:0.383728\n"
     ]
    }
   ],
   "source": [
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_test = xgb.DMatrix(x_test, label=y_test)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_test, 'test')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 100, watchlist, early_stopping_rounds=50, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now that the log loss is much better than before. I will now try with other parameters until I get a better log loss. I obtained these parameters modifying them lots of times and checking which obtained the lowest log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.477111\ttest-logloss:0.483283\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 50 rounds.\n",
      "[10]\ttrain-logloss:0.307731\ttest-logloss:0.342057\n",
      "[20]\ttrain-logloss:0.298864\ttest-logloss:0.34017\n",
      "[30]\ttrain-logloss:0.291109\ttest-logloss:0.338717\n",
      "[40]\ttrain-logloss:0.279607\ttest-logloss:0.335779\n",
      "[50]\ttrain-logloss:0.271199\ttest-logloss:0.3337\n",
      "[60]\ttrain-logloss:0.264348\ttest-logloss:0.332541\n",
      "[70]\ttrain-logloss:0.258186\ttest-logloss:0.33152\n",
      "[80]\ttrain-logloss:0.252167\ttest-logloss:0.329898\n",
      "[90]\ttrain-logloss:0.246838\ttest-logloss:0.328533\n",
      "[100]\ttrain-logloss:0.243106\ttest-logloss:0.327813\n",
      "[110]\ttrain-logloss:0.239892\ttest-logloss:0.327553\n",
      "[120]\ttrain-logloss:0.236237\ttest-logloss:0.326782\n",
      "[130]\ttrain-logloss:0.232911\ttest-logloss:0.326073\n",
      "[140]\ttrain-logloss:0.226967\ttest-logloss:0.324405\n",
      "[150]\ttrain-logloss:0.222231\ttest-logloss:0.32309\n",
      "[160]\ttrain-logloss:0.218936\ttest-logloss:0.322197\n",
      "[170]\ttrain-logloss:0.215858\ttest-logloss:0.32155\n",
      "[180]\ttrain-logloss:0.213443\ttest-logloss:0.320959\n",
      "[190]\ttrain-logloss:0.21211\ttest-logloss:0.320734\n",
      "[199]\ttrain-logloss:0.210903\ttest-logloss:0.320533\n"
     ]
    }
   ],
   "source": [
    "params['objective'] = 'binary:logistic'\n",
    "#params['objective'] = 'reg:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.5\n",
    "params['max_depth'] = 15\n",
    "params['alpha'] = 0.5\n",
    "params['lambda'] = 0.7\n",
    "\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_test = xgb.DMatrix(x_test, label=y_test)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_test, 'test')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 200, watchlist, early_stopping_rounds=50, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log loss after tuning is about 0.32. I will now try the algorithm in the testing set, and upload it to Kaggle to see the result, and the position in the Leaderboard. First I need to obtain the same features than before, but for the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions1 = data_test.question1.tolist()\n",
    "questions2 = data_test.question2.tolist()\n",
    "    \n",
    "clean_questions = {'question1_clean': cleanQuestions(questions1), 'question2_clean': cleanQuestions(questions2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3345831 unique questions in the dataset.\n"
     ]
    }
   ],
   "source": [
    "questions1_clean = clean_questions['question1_clean']\n",
    "questions2_clean = clean_questions['question2_clean']\n",
    "\n",
    "questions = []\n",
    "\n",
    "for i in range(len(questions1_clean)):\n",
    "    questions.append(str(questions1_clean[i]))\n",
    "    questions.append(str(questions2_clean[i]))\n",
    "    \n",
    "questions = set(questions)\n",
    "\n",
    "print(\"There are %d unique questions in the dataset.\" %len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_tokens = [nltk.tokenize.word_tokenize(str(i)) for i in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5)\n",
    "\n",
    "tfidf = vectorizer.fit_transform(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf = vectorizer.idf_\n",
    "idf_dict = dict(zip(vectorizer.get_feature_names(), idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question1_token = [nltk.tokenize.word_tokenize(i) for i in questions1_clean]\n",
    "question2_token = [nltk.tokenize.word_tokenize(i) for i in questions2_clean]\n",
    "\n",
    "shared_words = []\n",
    "total_words = []\n",
    "\n",
    "for i in range(len(question1_token)):\n",
    "    shared = []\n",
    "    total = []\n",
    "    for w in question1_token[i]:\n",
    "        total.append(w)\n",
    "        if w in question2_token[i]:\n",
    "            shared.append(w)\n",
    "    for w in question2_token[i]:\n",
    "        total.append(w)\n",
    "        if w in question1_token[i]:\n",
    "            shared.append(w)\n",
    "    shared_words.append(shared)\n",
    "    total_words.append(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shared_weights = []\n",
    "total_weights = []\n",
    "not_in_dict = []\n",
    "\n",
    "for i in range(len(shared_words)):\n",
    "    shared = []\n",
    "    total = []\n",
    "    for w in shared_words[i]:\n",
    "        try:\n",
    "            shared.append(idf_dict[w])\n",
    "        except:\n",
    "            not_in_dict.append(w)\n",
    "            shared.append(0)\n",
    "    for w in total_words[i]:\n",
    "        try:\n",
    "            total.append(idf_dict[w])\n",
    "        except:\n",
    "            not_in_dict.append(w)\n",
    "            shared.append(0)\n",
    "    shared_weights.append(shared)\n",
    "    total_weights.append(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = []\n",
    "\n",
    "for i in range(len(shared_weights)):\n",
    "    shared = 0\n",
    "    total = 0\n",
    "    for j in range(len(shared_weights[i])):\n",
    "        shared += shared_weights[i][j]\n",
    "    for j in range(len(total_weights[i])):\n",
    "        total += total_weights[i][j]\n",
    "    if total == 0:\n",
    "        weights.append(0)\n",
    "    else:\n",
    "        weights.append(shared / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shared_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.343789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.674897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.730260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.352380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.631104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shared_weights\n",
       "0        0.343789\n",
       "1        0.674897\n",
       "2        0.730260\n",
       "3        0.352380\n",
       "4        0.631104"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame(weights, columns=['shared_weights'])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\andres\\anaconda2\\envs\\udacity\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "c:\\users\\andres\\anaconda2\\envs\\udacity\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shared_weights</th>\n",
       "      <th>shared_count_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.343789</td>\n",
       "      <td>0.116667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.674897</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.730260</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.352380</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.631104</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shared_weights  shared_count_scaled\n",
       "0        0.343789             0.116667\n",
       "1        0.674897             0.133333\n",
       "2        0.730260             0.100000\n",
       "3        0.352380             0.033333\n",
       "4        0.631104             0.066667"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "shared_count = [len(x) for x in shared_words]\n",
    "shared_count_scaled = MinMaxScaler().fit_transform(shared_count)\n",
    "\n",
    "X['shared_count_scaled'] = shared_count_scaled\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\andres\\anaconda2\\envs\\udacity\\lib\\site-packages\\sklearn\\preprocessing\\data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "c:\\users\\andres\\anaconda2\\envs\\udacity\\lib\\site-packages\\sklearn\\preprocessing\\data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shared_weights</th>\n",
       "      <th>shared_count_scaled</th>\n",
       "      <th>len_dif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.343789</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.047170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.674897</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.018868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.730260</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.028302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.352380</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.631104</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.009434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shared_weights  shared_count_scaled   len_dif\n",
       "0        0.343789             0.116667  0.047170\n",
       "1        0.674897             0.133333  0.018868\n",
       "2        0.730260             0.100000  0.028302\n",
       "3        0.352380             0.033333  0.000000\n",
       "4        0.631104             0.066667  0.009434"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question1_len = [len(x) for x in question1_token]\n",
    "question2_len = [len(x) for x in question2_token]\n",
    "\n",
    "len_dif = []\n",
    "\n",
    "for i in range(len(question1_len)):\n",
    "    len_dif.append(abs(question1_len[i] - question2_len[i]))\n",
    "\n",
    "len_dif_scaled = MinMaxScaler().fit_transform(len_dif)\n",
    "\n",
    "X['len_dif'] = len_dif_scaled\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 2345796/2345796 [18:40<00:00, 2093.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shared_weights</th>\n",
       "      <th>shared_count_scaled</th>\n",
       "      <th>len_dif</th>\n",
       "      <th>z_match_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.343789</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.047170</td>\n",
       "      <td>0.384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.674897</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.440367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.730260</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0.584270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.352380</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.631104</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.009434</td>\n",
       "      <td>0.677419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shared_weights  shared_count_scaled   len_dif  z_match_ratio\n",
       "0        0.343789             0.116667  0.047170       0.384000\n",
       "1        0.674897             0.133333  0.018868       0.440367\n",
       "2        0.730260             0.100000  0.028302       0.584270\n",
       "3        0.352380             0.033333  0.000000       0.545455\n",
       "4        0.631104             0.066667  0.009434       0.677419"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_match_ratio = []\n",
    "\n",
    "for i in tqdm(range(len(data_test))):\n",
    "    z_match_ratio.append(diff_ratios(data_test.question1[i], data_test.question2[i]))\n",
    "                         \n",
    "X['z_match_ratio'] = z_match_ratio\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have all the features for the testing data, I will use the model estimated before to get the predicted values for each pair of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = bst.predict(xgb.DMatrix(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.001199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.093642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.230531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.441855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.514989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id  is_duplicate\n",
       "0        0      0.001199\n",
       "1        1      0.093642\n",
       "2        2      0.230531\n",
       "3        3      0.441855\n",
       "4        4      0.514989"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(data_test.test_id)\n",
    "\n",
    "submission['is_duplicate'] = pred\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will save my predictions as a csv and submit them to Kaggle to check the log loss obtained in their testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('sub/xgboost_predictor.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final score obtained in Kaggle is **0.44814**, which is much better than the one obtained with the naive predictor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
